#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Step3 Finetune Simple Evaluation - Evaluate only the content generated by fine-tuned models
Only evaluate original content, no rewriting improvement
"""

import time
import json
import os
import logging
import traceback
import re
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import sys

# Import necessary modules
from ollama_client import generate_text
from config import MODEL_CONFIG

# Initialize logging with finetune-specific names
def init_logging():
    os.makedirs("logs", exist_ok=True)
    
    # Configure main logger
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s [%(levelname)s] [%(filename)s:%(lineno)d] %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler("logs/step3_finetune.out", mode="w", encoding="utf-8")
        ]
    )
    
    # Add error handler
    err_handler = logging.FileHandler("logs/step3_finetune.err", mode="w", encoding="utf-8")
    err_handler.setLevel(logging.ERROR)
    logger = logging.getLogger()
    logger.addHandler(err_handler)
    
    return logging.getLogger(__name__)

logger = init_logging()

def ag2_group_review(section_name: str, section_text: str, referenced_papers: Optional[Dict] = None) -> Dict:
    """Use simplified AG2 multi-expert review to score individual sections"""
    logger.info(f"[AG2] Start group review: section={section_name}")

    # Construct reference context
    ref_context = ""
    if referenced_papers:
        lines = ["# Referenced Papers"]
        for title, meta in referenced_papers.items():
            cid = meta.get('citation_id', 'NA')
            lines.append(f"[{cid}] {title}")
            if meta.get('abstract'):
                lines.append(f"Abstract: {meta['abstract'][:200]}...")
        ref_context = "\n".join(lines)

    # Expert role definitions
    agents = ["technical_accuracy_agent", "clarity_agent", "structure_agent", "fact_checking_agent"]
    
    role_prompts = {
        "technical_accuracy_agent": "You are a technical accuracy expert, evaluating content factual correctness, terminology precision, and technical depth.",
        "clarity_agent": "You are a clarity and readability expert, evaluating logical clarity, fluency of expression, and comprehensibility.",
        "structure_agent": "You are a structural organization expert, evaluating paragraph organization, overall structure, and logical transitions.",
        "fact_checking_agent": "You are a fact and citation verification expert, evaluating citation accuracy and factual support."
    }

    # Collect expert reviews
    agent_reviews = {}
    
    for agent in agents:
        try:
            prompt = f"""You are {role_prompts[agent]}
Task: Provide professional scoring for the {section_name} section of the following academic text

Scoring criteria:
- Technical accuracy expert: Evaluate technical_depth (0-1 points)
- Clarity expert: Evaluate clarity (0-1 points)  
- Structure expert: Evaluate structure (0-1 points)
- Citation verification expert: Evaluate citation_accuracy (0-1 points)

Please output in JSON format:
{{
  "score": 0.85,
  "reason": "Detailed scoring reason"
}}

{f"References: {ref_context}" if referenced_papers and agent == "fact_checking_agent" else ""}

Text to evaluate:
{section_text}
"""

            response = generate_text(
                prompt=prompt,
                model=MODEL_CONFIG.small_model,
                system="You are an academic review expert, strictly output scoring in JSON format.",
                temperature=0.2,
                max_tokens=500,
                task_type="evaluation"
            )
            
            # Parse JSON
            json_match = re.search(r'\{[^}]*"score"[^}]*\}', response)
            if json_match:
                score_data = json.loads(json_match.group())
                agent_reviews[agent] = {
                    "score": float(score_data.get("score", 0.0)),
                    "reason": score_data.get("reason", "No reason provided"),
                    "raw_response": response
                }
                logger.info(f"[AG2] {agent} review completed: {score_data.get('score', 0.0)}")
            else:
                agent_reviews[agent] = {
                    "score": 0.0,
                    "reason": "Failed to parse response",
                    "raw_response": response
                }
                logger.error(f"[AG2] {agent} review parsing failed")
                
        except Exception as e:
            logger.error(f"[AG2] {agent} review failed: {e}")
            agent_reviews[agent] = {
                "score": 0.0,
                "reason": f"Error: {str(e)}",
                "raw_response": ""
            }

    # Summarize scoring results
    panel_scores = {
        "technical_depth": agent_reviews.get("technical_accuracy_agent", {}).get("score"),
        "clarity": agent_reviews.get("clarity_agent", {}).get("score"),
        "structure": agent_reviews.get("structure_agent", {}).get("score"),
        "citation_accuracy": agent_reviews.get("fact_checking_agent", {}).get("score") if referenced_papers else None
    }
    
    panel_reasons = {
        "technical_depth": agent_reviews.get("technical_accuracy_agent", {}).get("reason"),
        "clarity": agent_reviews.get("clarity_agent", {}).get("reason"),
        "structure": agent_reviews.get("structure_agent", {}).get("reason"),
        "citation_accuracy": agent_reviews.get("fact_checking_agent", {}).get("reason") if referenced_papers else None
    }

    result = {
        "section_name": section_name,
        "original_content": section_text,
        "panel_scores": panel_scores,
        "panel_reasons": panel_reasons,
        "agent_reviews": agent_reviews
    }

    logger.info(f"[AG2] Review completed: scores={panel_scores}")
    return result

def evaluate_section(section_name: str, section_content: str, referenced_papers: Optional[Dict] = None) -> Dict:
    """Evaluate a single section"""
    logger.info(f"Evaluating section: {section_name}")
    
    try:
        # Use AG2 group review
        review_result = ag2_group_review(section_name, section_content, referenced_papers)
        
        return {
            "panel_scores": review_result["panel_scores"],
            "panel_reasons": review_result["panel_reasons"],
            "agent_reviews": review_result["agent_reviews"]
        }
        
    except Exception as e:
        logger.error(f"Failed to evaluate section {section_name}: {e}")
        return {
            "error": str(e),
            "panel_scores": {
                "technical_depth": None,
                "clarity": None,
                "structure": None,
                "citation_accuracy": None
            }
        }

def load_chapter_data(file_path: Path) -> Dict:
    """Load chapter data"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Failed to load file {file_path}: {e}")
        return {}

def evaluate_chapter(chapter_file: Path) -> bool:
    """Evaluate a single chapter file"""
    logger.info(f"Starting to evaluate chapter file: {chapter_file}")
    
    try:
        # Load chapter data
        chapter_data = load_chapter_data(chapter_file)
        if not chapter_data:
            return False
        
        # Extract basic information
        chapter_number = chapter_data.get("metadata", {}).get("report_number", 0)
        question = chapter_data.get("question", "")
        sections = chapter_data.get("sections", {})
        referenced_papers = chapter_data.get("referenced_papers", {})
        
        logger.info(f"Chapter {chapter_number}: {question}")
        
        # Evaluate each section
        section_evaluations = {}
        
        for section_name, section_content in sections.items():
            if section_name == "REFERENCES":
                continue  # Skip references section
                
            logger.info(f"Evaluating section: {section_name}")
            evaluation = evaluate_section(section_content, section_name, referenced_papers)
            section_evaluations[section_name] = evaluation
        
        # Calculate overall scores
        overall_scores = {
            "technical_depth": None,
            "clarity": None,
            "structure": None,
            "citation_accuracy": None
        }
        
        # Aggregate scores from each section
        for metric in overall_scores.keys():
            scores = []
            for section_eval in section_evaluations.values():
                if isinstance(section_eval, dict) and "panel_scores" in section_eval:
                    score = section_eval["panel_scores"].get(metric)
                    if score is not None:
                        scores.append(score)
            
            if scores:
                overall_scores[metric] = sum(scores) / len(scores)
        
        # Build result
        # Create result dictionary
        result = {
            "chapter_file": str(chapter_file),
            "chapter_number": chapter_number,
            "question": chapter_data.get("question", ""),
            "domain": chapter_data.get("domain", ""),
            "timestamp": chapter_data.get("timestamp", ""),
            "metadata": chapter_data.get("metadata", {}),
            "section_evaluations": section_evaluations,
            "overall_scores": overall_scores,
            "evaluation_timestamp": datetime.now().isoformat(),
            "model_type": chapter_data.get("metadata", {}).get("model", "unknown")
        }
        
        # Save evaluation results
        output_dir = Path("output_finetune")
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"chapter_{chapter_number}_evaluation_finetune_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Evaluation results saved: {output_file}")
        return True
        
    except Exception as e:
        logger.error(f"Failed to evaluate chapter {chapter_file}: {e}")
        logger.error(traceback.format_exc())
        return False

def get_all_chapter_files() -> List[Path]:
    """Get all chapter files"""
    chapters_dir = Path("initial_chapters_finetune")
    if not chapters_dir.exists():
        raise FileNotFoundError(f"Directory {chapters_dir} does not exist")
    return sorted(chapters_dir.glob("chapter_*_finetune.json"))

def main():
    """Main function"""
    try:
        logger.info("=== Starting fine-tuned model content evaluation ===")
        
        # Get all chapter files
        chapter_files = get_all_chapter_files()
        if not chapter_files:
            logger.error("No chapter files found")
            return
        
        logger.info(f"Found {len(chapter_files)} chapter files")
        
        success_count = 0
        total_count = len(chapter_files)
        
        for chapter_file in chapter_files:
            if evaluate_chapter(chapter_file):
                success_count += 1
            
            # Brief pause to avoid API overload
            time.sleep(1)
        
        logger.info(f"=== Evaluation completed: {success_count}/{total_count} files successful ===")
        
    except Exception as e:
        logger.error(f"Main program execution failed: {e}")
        logger.error(traceback.format_exc())

if __name__ == "__main__":
    main()
